# SmartStorage



# Ollama
Il est extremenment recommand√© d'avoir une machine avec un gpu pour avoir des performances acceptables avec les LLM.
Vous pouver : 
- installer Ollama sur votre machine en suivant les instructions sur le site officiel de Ollama : https://ollama.com/
- utiliser Ollama dans un docker:
    - sur Windows :
        - CUDA on WSL : https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl
    - sur Linux : 
        - NVIDIA Container Toolkit : https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
